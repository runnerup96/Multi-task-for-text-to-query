{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb7c05-2d12-4300-a9dd-a00e75dc8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "from tqdm import notebook\n",
    "from collections import Counter\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "sys.path.append(\"\")#path to code\n",
    "\n",
    "from split_logic.grammar.sparql_parser import SPARQLParser\n",
    "from split_logic.grammar.sql_parser import SQLParser\n",
    "from split_logic.grammar import atom_and_compound_cache\n",
    "\n",
    "importlib.reload(atom_and_compound_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206e902-afed-47dc-87da-dd86d2e92318",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = ''#path to project\n",
    "EXPERIEMENTS_DATAPATH = ''#path to experiements folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fbc1f-6935-4abc-9e44-b768e92cafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_dataset = json.load(open('', 'r'))#path to whole sql dataset\n",
    "sparql_list = [sample[\"masked_query\"] for sample in sparql_dataset]\n",
    "sparql_parser = SPARQLParser(sparql_list)\n",
    "parser_dict = {'wikidata': sparql_parser}\n",
    "compound_path = os.path.join(PROJECT_PATH, 'dataset/lcquad/tmcd_split')\n",
    "cached_sparql_parser = atom_and_compound_cache.AtomAndCompoundCache(parser_dict, query_key_name = None,\n",
    "                                                                    kb_id_key_name=None,\n",
    "                                                                    return_compound_list_flag = False,\n",
    "                                                                   compound_cache_path=compound_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84eebb4-4beb-4fbd-817b-21414ebbb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dataset = json.load(open())#path to whole sparql dataset\n",
    "db2attr_dict = json.load(open(os.path.join(PROJECT_PATH,\n",
    "                                           \"dataset/wikisql/table_id2new_attrs_for_parsing.json\")))\n",
    "\n",
    "compound_path = os.path.join(PROJECT_PATH, 'dataset/wikisql/tmcd_split')\n",
    "parser_dict = dict()\n",
    "for sample in tqdm.tqdm(sql_dataset, total=len(sql_dataset)):\n",
    "    db_id = sample['kb_id']\n",
    "    db_attributes = db2attr_dict[db_id]\n",
    "    if db_id not in parser_dict:\n",
    "        parser_instance = SQLParser(db_attributes)\n",
    "        parser_dict[db_id] = parser_instance\n",
    "        \n",
    "        \n",
    "cached_sql_parser = atom_and_compound_cache.AtomAndCompoundCache(parser_dict,\n",
    "                                                                 query_key_name=None, kb_id_key_name=None,\n",
    "                                                                 return_compound_list_flag=False,\n",
    "                                                                 compound_cache_path=compound_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12443e18-2081-4caa-bb7e-94b16144155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PARSER = cached_sparql_parser\n",
    "FOLDER_NAME = 'tmcd_sparql_hirm_control_v5'\n",
    "FILE_NAME = 'epoch_94_tm_0.85_t5_predictions.json'\n",
    "\n",
    "prediction_file = json.load(open(os.path.join(EXPERIEMENTS_DATAPATH, FOLDER_NAME, FILE_NAME), 'r'))\n",
    "true_questions, predicted_queries, true_queries = prediction_file['input_questions'], prediction_file['predicted_queries'], prediction_file['true_queries']\n",
    "\n",
    "DATASET_FOLDER_PATH = 'dataset/lcquad/tmcd_split'\n",
    "TEST_FILE_NAME = 'english_test_split_coef_0.1.json'\n",
    "TRAIN_FILE_NAME = 'english_train_split_coef_0.1.json'\n",
    "\n",
    "test_set = json.load(open(os.path.join(PROJECT_PATH, DATASET_FOLDER_PATH, TEST_FILE_NAME), 'r'))\n",
    "train_set = json.load(open(os.path.join(PROJECT_PATH, DATASET_FOLDER_PATH, TRAIN_FILE_NAME), 'r'))\n",
    "\n",
    "assert len(test_set) == len(true_queries)\n",
    "for i in range(len(test_set)):\n",
    "    assert test_set[i]['masked_query'] == true_queries[i]\n",
    "    \n",
    "true_queries_kb_list = []\n",
    "for i in range(len(test_set)):\n",
    "    true_queries_kb_list.append(test_set[i]['kb_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6901c-3c73-4a46-b87e-858ffe807470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f4efa3f-21f4-4f22-a4a5-c6fe160f5973",
   "metadata": {},
   "source": [
    "### Train/test and prediction parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ea3d1-4f46-411b-8629-531692f8a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries_components = []\n",
    "for sample in notebook.tqdm(train_set, total=len(train_set)):\n",
    "    query, question, kb_id = sample['masked_query'], sample['question'], sample['kb_id']\n",
    "    compounds = DATASET_PARSER.get_compounds(query, kb_id)\n",
    "    atoms = DATASET_PARSER.get_atoms(query, kb_id)\n",
    "    result_dict = {\n",
    "        'question': question,\n",
    "        'query': query,\n",
    "        'compound': compounds,\n",
    "        'atoms': atoms\n",
    "    }\n",
    "    train_queries_components.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208820d-384b-4878-a1c3-908ee383a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_queries_components, test_pred_queries_components = [], []\n",
    "for question, true_query, pred_query, kb_id in notebook.tqdm(zip(true_questions, true_queries, predicted_queries, true_queries_kb_list), total=len(true_queries)):\n",
    "    true_compounds = DATASET_PARSER.get_compounds(true_query, kb_id)\n",
    "    true_atoms = DATASET_PARSER.get_atoms(true_query, kb_id)\n",
    "    \n",
    "    pred_compounds = DATASET_PARSER.get_compounds(pred_query, kb_id)\n",
    "    pred_atoms = DATASET_PARSER.get_atoms(pred_query, kb_id)\n",
    "    \n",
    "    true_result_dict = {\n",
    "        'question': question,\n",
    "        'query': true_query,\n",
    "        'compound': true_compounds,\n",
    "        'atoms': true_atoms\n",
    "    }\n",
    "    test_true_queries_components.append(true_result_dict)\n",
    "    \n",
    "    pred_result_dict = {\n",
    "        'question': question,\n",
    "        'query': pred_query,\n",
    "        'compound': pred_compounds,\n",
    "        'atoms': pred_atoms\n",
    "    }\n",
    "    test_pred_queries_components.append(pred_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9f193-fac9-4331-8099-8b4351fdbb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f3d1d18-215a-4cfb-b75d-4446a552beb8",
   "metadata": {},
   "source": [
    "### Predicted compound accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9363e3f-33a7-4e8e-bb99-0cbbf52e26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_compounds(true_comp_d, pred_comp_d):\n",
    "    compound_hits_dict = dict()\n",
    "    for key in true_comp_d:\n",
    "        true_compound_counter = Counter(true_comp_d[key])\n",
    "        total_hits = sum(true_compound_counter.values())\n",
    "        compound_match_hits = 0\n",
    "        for compound in pred_comp_d[key]:\n",
    "            if compound in true_compound_counter:\n",
    "                compound_match_hits += 1\n",
    "        \n",
    "        if total_hits == 0 and total_hits == compound_match_hits:\n",
    "            compound_acc = 1.0\n",
    "        else:\n",
    "            compound_acc = compound_match_hits / total_hits\n",
    "            \n",
    "        compound_hits_dict[key] = compound_acc\n",
    "    return compound_hits_dict\n",
    "\n",
    "def compare_atoms(true_atoms, pred_atoms):\n",
    "    true_atoms_set = set(true_atoms)\n",
    "    pred_atoms_set = set(pred_atoms)\n",
    "    return len(true_atoms_set.intersection(pred_atoms_set)) / len(true_atoms)\n",
    "\n",
    "def test():\n",
    "    print(compare_atoms(test_pred_queries_components[0]['atoms'], test_true_queries_components[0]['atoms'])) \n",
    "    print(test_pred_queries_components[0]['compound'])\n",
    "    print(test_true_queries_components[0]['compound'])\n",
    "    print(compare_compounds(test_pred_queries_components[0]['compound'], test_true_queries_components[0]['compound']))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66125e-e2a3-4aa7-988f-7b50ca7be17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_result_dict = {key: 0 for key in DATASET_PARSER.parsers_env_list}\n",
    "atom_acc = 0\n",
    "\n",
    "for true_comp, pred_comp in zip(test_true_queries_components, test_pred_queries_components):\n",
    "    compound_acc = compare_compounds(true_comp['compound'], pred_comp['compound'])\n",
    "    for key in compound_result_dict:\n",
    "        compound_result_dict[key] += compound_acc[key]\n",
    "        \n",
    "    atom_acc += compare_atoms(true_comp['atoms'], pred_comp['atoms'])\n",
    "    \n",
    "em_match_prob = 1\n",
    "for key in compound_result_dict:\n",
    "    compound_result_dict[key] = round(compound_result_dict[key] / len(test_true_queries_components), 3)\n",
    "    em_match_prob *= compound_result_dict[key]\n",
    "\n",
    "em_match_prob = round(em_match_prob, 3)\n",
    "atom_acc =  round(atom_acc / len(test_true_queries_components), 3)\n",
    "\n",
    "print('Compound accuracy: ', compound_result_dict)\n",
    "print('Atom accuracy: ', atom_acc)\n",
    "print('EM Match prob: ', em_match_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac71d1-4a23-42f6-9d37-bbec03e8f503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8834bd0-f4f9-4e13-a098-d1304e885109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc381e00-4235-4d3c-8617-f8322a854654",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Syntax correctness of predicted compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f6291-7e95-4c18-94f7-a9dd1ac7d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_syntactic_structure(true_comp_d, pred_comp_d):\n",
    "    syntax_score = 0\n",
    "    expected_compounds = []\n",
    "    for key in true_comp_d:\n",
    "        if len(true_comp_d[key]) > 0:\n",
    "            expected_compounds.append(key)\n",
    "    \n",
    "    for key in expected_compounds:\n",
    "        if len(true_comp_d[key]) > 0:\n",
    "            syntax_score += 1\n",
    "    \n",
    "    syntax_score /= len(expected_compounds)\n",
    "    return syntax_score\n",
    "    \n",
    "\n",
    "def test():\n",
    "    print(test_pred_queries_components[0]['compound'])\n",
    "    print(test_true_queries_components[0]['compound'])\n",
    "    print(compare_syntactic_structure(test_pred_queries_components[0]['compound'], test_true_queries_components[0]['compound']))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24290d76-588f-4774-be08-8b452677ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_syntax_score = 0\n",
    "for true_comp, pred_comp in zip(test_true_queries_components, test_pred_queries_components):\n",
    "    syntax_score = compare_syntactic_structure(true_comp['compound'], pred_comp['compound'])\n",
    "    total_syntax_score += syntax_score\n",
    "    \n",
    "print('Syntax score: ', round(total_syntax_score / len(test_true_queries_components), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56404a-49a7-40ea-869f-a98e82558b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "523233d9-2b56-4b9e-b7e0-75cb0b39bf95",
   "metadata": {},
   "source": [
    "### OOD compound accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb930bcb-8fb5-4eb3-a8ef-880a09305621",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_structures = set()\n",
    "for sample in train_queries_components:\n",
    "    train_compounds = sample['compound']\n",
    "    for compound in train_compounds:\n",
    "        train_structures.update(set(train_compounds[compound]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34312975-07f1-4ac6-89c0-5ca777670b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e17360-26d8-4098-9824-e88608be8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_comp_acc_dict = {key: {'hits': 0, 'total': 0, 'sample_idx_list': [], 'hit_compound_example_set': set(), 'total_compound_example_set': set()} for key in compound_result_dict}\n",
    "for idx, (true_sample, pred_sample) in enumerate(zip(test_true_queries_components, test_pred_queries_components)):\n",
    "    test_compound, pred_compound = true_sample['compound'], pred_sample['compound']\n",
    "    for t_c in test_compound:\n",
    "        test_c_l, pred_c_l = test_compound[t_c], pred_compound[t_c]\n",
    "        for test_c in test_c_l:\n",
    "            if test_c not in train_structures:\n",
    "                if test_c in pred_c_l:\n",
    "                    oov_comp_acc_dict[t_c]['hits'] += 1\n",
    "                    oov_comp_acc_dict[t_c]['sample_idx_list'].append(idx)\n",
    "                    oov_comp_acc_dict[t_c]['hit_compound_example_set'].add(pred_c)\n",
    "                oov_comp_acc_dict[t_c]['total'] += 1\n",
    "                oov_comp_acc_dict[t_c]['total_compound_example_set'].add(pred_c)\n",
    "\n",
    "print('OOV compound accuracy per component: ')\n",
    "for key in oov_comp_acc_dict:\n",
    "    hits = oov_comp_acc_dict[key]['hits']\n",
    "    print(f'total hits {key}: ', hits)\n",
    "    total = oov_comp_acc_dict[key]['total']\n",
    "    print(f'total ood {key}: ', total)\n",
    "    acc = 0 if total == 0 else hits/total\n",
    "    print(f'Compound ood accuracy {key} = {round(acc, 3)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586422d1-7b9e-48c3-bb43-e4ff6bd76266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efa8dc-1710-49cc-8d97-04abd6af7ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d914c20-005a-4ece-afec-290fd4e9d774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0ee7f-fc43-455c-bd1e-80a7a29faeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ac3cee-f897-445f-b015-250d53a9936c",
   "metadata": {},
   "source": [
    "### Текущее распределение train/test по компаундам/атомам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cfc60-8921-4aa4-b926-0db38e5973b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = json.load(open('/Users/somov-od/Documents/phd/projects/CompGen/text2query/dataset/lcquad/tmcd_split/english_train_split_coef_0.1.json'))\n",
    "test_dataset = json.load(open('/Users/somov-od/Documents/phd/projects/CompGen/text2query/dataset/lcquad/tmcd_split/english_test_split_coef_0.1.json'))\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b507067-035c-4a8c-b002-26e49bbb56e0",
   "metadata": {},
   "source": [
    "1) Насколько LM задача сама по себе генерирует новые верные стурктуры?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a462da-7658-4de9-bb83-fb457cabd78a",
   "metadata": {},
   "source": [
    "### TMCD Environment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522963e-de64-437a-9d87-867d5a337e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937bf8c-539a-4428-bfbb-30ed0c75686f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1f2dd-4d27-479a-b51b-9f1521403d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### посмотрим когда у нас абсолютно пустые select_compound + triplet_compound - это значит что у нас точно неверная грамматика генерации\n",
    "\n",
    "k = 0\n",
    "failed_grammar = []\n",
    "for idx, sample in enumerate(test_pred_queries_components):\n",
    "    compound_dict = sample['pred_compound']\n",
    "    # если первое сгенеренное это ask - смотрим только на compound\n",
    "    if 'ask' == sample['pred_query'].split()[0]:\n",
    "        if len(compound_dict['triplet_compound']) > 0:\n",
    "            k += 1\n",
    "        else:\n",
    "            failed_grammar.append([idx, sample])\n",
    "    else:w\n",
    "        if  len(compound_dict['select_compound']) > 0 and len(compound_dict['triplet_compound']) > 0 :\n",
    "            k += 1\n",
    "        else:\n",
    "            failed_grammar.append([idx, sample])\n",
    "        \n",
    "# вывод - у нас 68% генерируется даже не всегда грамматически корректный запрос. Получается нам нужно в том числе уметь генерить грамматически верные запросы тоже.\n",
    "k / len(test_pred_queries_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412432ed-65e1-4af0-b43d-19486c8bf459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failed_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62930a-4460-4aa2-b294-1b05729ad0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_true_queries_components[260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717848c9-6a98-460e-b4d9-feb8b236f4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failed_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4dfa94-9b01-4da1-bc0e-f6e1727928f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'ask' in failed_grammar[0]['pred_query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a289c4-9ef4-48a4-ad6d-f2d310d93383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38489585-b6f3-45e0-b53c-8960a98edeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_dict = {key: 0 for key in train_queries_components[0]['compound']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc8587-b902-4131-9d0b-308f33526ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как у нас пересекаются структуры?\n",
    "\n",
    "train_structures = set()\n",
    "for sample in train_queries_components:\n",
    "    train_compounds = sample['compound']\n",
    "    for compound in train_compounds:\n",
    "        train_structures.update(set(train_compounds[compound]))\n",
    "        \n",
    "len(train_structures)\n",
    "\n",
    "test_structures, pred_structures = set(), set()\n",
    "for true_sample, pred_sample in zip(test_true_queries_components, test_pred_queries_components):\n",
    "    test_compound, pred_compound = true_sample['compound'], pred_sample['pred_compound']\n",
    "    for t_c, p_c in zip(test_compound, pred_compound):\n",
    "        test_structures.update(set(test_compound[t_c]))\n",
    "        pred_structures.update(set(pred_compound[p_c]))\n",
    "        \n",
    "len(test_structures), len(pred_structures)\n",
    "\n",
    "\n",
    "print('Пересечение трейн структур и теста:', len(train_structures.intersection(test_structures)) / len(train_structures))\n",
    "\n",
    "print('Пересечение трейн структур и предикта:', len(train_structures.intersection(pred_structures)) / len(train_structures))\n",
    "\n",
    "print('Пересечение тест структур и предикта:', len(test_structures.intersection(pred_structures)) / len(test_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f591b9-b08f-433d-ba5c-2b4bdf002a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# У нас есть новые структуры в предикте, которых нет в трейне. Какое качесвто модели на них?\n",
    "\n",
    "\n",
    "oov_comp_acc_dict = {key: {'hits': 0, 'total': 0, 'sample_idx_list': [], 'hit_compound_example_set': set(), 'total_compound_example_set': set()} for key in compound_dict}\n",
    "for idx, (true_sample, pred_sample) in enumerate(zip(test_true_queries_components, test_pred_queries_components)):\n",
    "    test_compound, pred_compound = true_sample['compound'], pred_sample['pred_compound']\n",
    "    # для каждой структуре в запросе\n",
    "    for t_c in test_compound:\n",
    "        test_c_l, pred_c_l = test_compound[t_c], pred_compound[t_c]\n",
    "        # смотрим на предсказанные структуры\n",
    "        for pred_c in pred_c_l:\n",
    "            # модель его не видела на обучении\n",
    "            if pred_c not in train_structures:\n",
    "                # при этом мы предсказали его верно\n",
    "                if pred_c in test_c_l:\n",
    "                    oov_comp_acc_dict[t_c]['hits'] += 1\n",
    "                    oov_comp_acc_dict[t_c]['sample_idx_list'].append(idx)\n",
    "                    oov_comp_acc_dict[t_c]['hit_compound_example_set'].add(pred_c)\n",
    "                oov_comp_acc_dict[t_c]['total'] += 1\n",
    "                oov_comp_acc_dict[t_c]['total_compound_example_set'].add(pred_c)\n",
    "\n",
    "print('OOV compound accuracy per component: ')\n",
    "for key in oov_comp_acc_dict:\n",
    "    hits = oov_comp_acc_dict[key]['hits']\n",
    "    total = oov_comp_acc_dict[key]['total']\n",
    "    acc = 0 if total == 0 else hits/total\n",
    "    print(f'{key} = {round(acc, 3)}')\n",
    "    print(f'Total compounds for {key}: ', total)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "# вывод - модель не может обощить на select структуры, которые не видела на этапе обучения. Может обощить на triplet - егр \n",
    "# filter и order у нас как раз повторяются в трейне/тесте - поэтому тут они 0\n",
    "\n",
    "# Из 2к придуманных структур, мы только 30% генерим верно. \n",
    "print(f\"{len(oov_comp_acc_dict['triplet_compound']['hit_compound_example_set']) / len(oov_comp_acc_dict['triplet_compound']['total_compound_example_set'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce89cf-b89f-49d7-8d30-b48b859598ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb062b-4431-4ffb-8bbe-d1ad2edd724c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59027edb-157c-4cb3-9900-648c881ef9d8",
   "metadata": {},
   "source": [
    "### Инженерия environment\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05ffd9-64e9-4163-b68e-3ae3329f0a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8aacb-0941-4181-9b69-d1b27bee0de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89a431-8c54-4f12-bd99-8a805626865d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41e373-c355-402b-b926-64e7c54223df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99885ed4-41b9-40a8-8a14-639aa46f0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_components_counter = {key: 0 for key in train_queries_components[0]['compound']}\n",
    "for sample in train_queries_components:\n",
    "    compound_dict = sample['compound']\n",
    "    for key in compound_dict:\n",
    "        train_components_counter[key] += len(compound_dict[key])\n",
    "    \n",
    "for key in compound_dict:\n",
    "    train_components_counter[key] /= len(train_queries_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883226d1-f6b4-4d16-8000-1111b0cc9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_components_counter = {key: 0 for key in train_queries_components[0]['compound']}\n",
    "for sample in test_true_queries_components:\n",
    "    compound_dict = sample['compound']\n",
    "    for key in compound_dict:\n",
    "        test_components_counter[key] += len(compound_dict[key])\n",
    "        \n",
    "for key in compound_dict:\n",
    "    test_components_counter[key] /= len(test_true_queries_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815dee80-6e59-4139-927e-ff01d8572060",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_components_counter = {key: 0 for key in train_queries_components[0]['compound']}\n",
    "for sample in test_pred_queries_components:\n",
    "    compound_dict = sample['pred_compound']\n",
    "    for key in compound_dict:\n",
    "        test_pred_components_counter[key] += len(compound_dict[key])\n",
    "        \n",
    "for key in compound_dict:\n",
    "    test_pred_components_counter[key] /= len(test_pred_queries_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb52e0-f901-4c1f-9fdf-cf39b17628d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe025e-9fb4-4287-8693-b05282494910",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_components_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace3579-de6d-4e95-bf37-87d3d004adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "plt.figure(figsize=(15, 6), dpi=80)\n",
    "X_axis = np.arange(len(train_components_counter))\n",
    "X_keys = list(train_components_counter.keys())\n",
    "train_vals = list(train_components_counter.values())\n",
    "test_vals = list(test_components_counter.values())\n",
    "pred_vals = list(test_pred_components_counter.values())\n",
    "  \n",
    "width = 0.2\n",
    "plt.bar(X_axis, train_vals, width, label = 'Train')\n",
    "plt.bar(X_axis + width, test_vals, width, label = 'Test')\n",
    "plt.bar(X_axis + width*2, pred_vals, width, label = 'Pred')\n",
    "  \n",
    "plt.xticks(X_axis, X_keys)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3d2c6-30e7-46b4-9e49-49f7edfb808c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
